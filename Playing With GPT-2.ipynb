{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Models with GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 12:29:21.838031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-13 12:29:22.055732: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-13 12:29:22.807231: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-13 12:29:22.807353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-13 12:29:22.807367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's play with gpt2 and gpt2-xl\n",
    "Note that we can use the Auto functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4048e368a204409b8f6050cad477dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ac1f13e490458d87a5a3aa64794ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a88f4568f864ed481af7e4aceb59293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e9d78d21de40beaec496216a42ef97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c6f9e31bb845a18539f7e7d9052a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "# model_name = 'gpt2-xl'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sequence: \n",
      "Machine learning with PyTorch can do amazing\n"
     ]
    }
   ],
   "source": [
    "seq = \"Machine learning with PyTorch can do amazing\"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[37573,  4673,   351,  9485, 15884,   354,   460,   466,  4998]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(seq, return_tensors=\"pt\").to(device)\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token IDs and their words: \n",
      "tensor(37573, device='cuda:0') Machine\n",
      "tensor(4673, device='cuda:0')  learning\n",
      "tensor(351, device='cuda:0')  with\n",
      "tensor(9485, device='cuda:0')  Py\n",
      "tensor(15884, device='cuda:0') Tor\n",
      "tensor(354, device='cuda:0') ch\n",
      "tensor(460, device='cuda:0')  can\n",
      "tensor(466, device='cuda:0')  do\n",
      "tensor(4998, device='cuda:0')  amazing\n"
     ]
    }
   ],
   "source": [
    "input_ids = inputs[\"input_ids\"]  # just IDS, no attn mask\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in input_ids[0]:\n",
    "  word = tokenizer.decode(id)\n",
    "  print(id, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's run it through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All logits for next word: \n",
      "tensor([[-114.9653, -118.0909, -123.3015,  ..., -124.5989, -127.7998,\n",
      "         -118.4347]], device='cuda:0')\n",
      "torch.Size([1, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits[:, -1, :]\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All probabilities: \n",
      "tensor([[2.7682e-05, 1.2155e-06, 6.6349e-09,  ..., 1.8128e-09, 7.3829e-11,\n",
      "         8.6184e-07]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "probs = torch.softmax(logits, dim=-1)\n",
    "print(\"\\nAll probabilities: \")\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Token ID</th>\n",
       "      <td>1243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logits</th>\n",
       "      <td>tensor(-104.5570)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Probability</th>\n",
       "      <td>tensor(0.9172)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Predicted Word</th>\n",
       "      <td>things</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Value\n",
       "Token ID                     1243\n",
       "Logits          tensor(-104.5570)\n",
       "Probability        tensor(0.9172)\n",
       "Predicted Word             things"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_id = torch.argmax(logits).item()\n",
    "pred_word = tokenizer.decode(pred_id)\n",
    "pd.DataFrame([pred_id, logits[0, pred_id].cpu(), probs[0, pred_id].cpu(), pred_word], \n",
    "              index=['Token ID', 'Logits', 'Probability', 'Predicted Word'], columns =['Value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look a bit closer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Transformers are the</td>\n",
       "      <td>vern (14.87%)</td>\n",
       "      <td>(11.98%)</td>\n",
       "      <td>ids (7.52%)</td>\n",
       "      <td>ices (4.99%)</td>\n",
       "      <td>urs (4.02%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transformers are the vern</td>\n",
       "      <td>acular (94.56%)</td>\n",
       "      <td>al (3.49%)</td>\n",
       "      <td>ier (1.01%)</td>\n",
       "      <td>iers (0.16%)</td>\n",
       "      <td>us (0.08%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformers are the vernacular</td>\n",
       "      <td>of (28.65%)</td>\n",
       "      <td>for (17.40%)</td>\n",
       "      <td>term (5.95%)</td>\n",
       "      <td>name (5.18%)</td>\n",
       "      <td>words (2.47%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Transformers are the vernacular of</td>\n",
       "      <td>the (22.59%)</td>\n",
       "      <td>all (1.54%)</td>\n",
       "      <td>a (1.45%)</td>\n",
       "      <td>our (0.75%)</td>\n",
       "      <td>this (0.69%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Transformers are the vernacular of the</td>\n",
       "      <td>game (1.31%)</td>\n",
       "      <td>time (1.14%)</td>\n",
       "      <td>world (1.08%)</td>\n",
       "      <td>modern (0.92%)</td>\n",
       "      <td>ancient (0.71%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Transformers are the vernacular of the game</td>\n",
       "      <td>. (25.34%)</td>\n",
       "      <td>, (22.62%)</td>\n",
       "      <td>'s (8.07%)</td>\n",
       "      <td>and (5.68%)</td>\n",
       "      <td>world (4.95%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Transformers are the vernacular of the game.</td>\n",
       "      <td>\\n (15.49%)</td>\n",
       "      <td>They (11.45%)</td>\n",
       "      <td>The (8.13%)</td>\n",
       "      <td>In (2.92%)</td>\n",
       "      <td>It (2.64%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Transformers are the vernacular of the game.\\n</td>\n",
       "      <td>\\n (99.65%)</td>\n",
       "      <td>The (0.04%)</td>\n",
       "      <td>A (0.02%)</td>\n",
       "      <td>I (0.01%)</td>\n",
       "      <td>In (0.01%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Transformers are the vernacular of the game.\\n\\n</td>\n",
       "      <td>The (9.98%)</td>\n",
       "      <td>In (3.43%)</td>\n",
       "      <td>Contents (3.10%)</td>\n",
       "      <td>A (2.06%)</td>\n",
       "      <td>They (1.84%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Transformers are the vernacular of the game.\\n...</td>\n",
       "      <td>game (5.70%)</td>\n",
       "      <td>first (1.94%)</td>\n",
       "      <td>main (1.30%)</td>\n",
       "      <td>player (1.04%)</td>\n",
       "      <td>\" (0.96%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input         Choice 1  \\\n",
       "0                              Transformers are the     vern (14.87%)   \n",
       "1                          Transformers are the vern  acular (94.56%)   \n",
       "2                    Transformers are the vernacular      of (28.65%)   \n",
       "3                 Transformers are the vernacular of     the (22.59%)   \n",
       "4             Transformers are the vernacular of the     game (1.31%)   \n",
       "5        Transformers are the vernacular of the game       . (25.34%)   \n",
       "6       Transformers are the vernacular of the game.      \\n (15.49%)   \n",
       "7     Transformers are the vernacular of the game.\\n      \\n (99.65%)   \n",
       "8   Transformers are the vernacular of the game.\\n\\n      The (9.98%)   \n",
       "9  Transformers are the vernacular of the game.\\n...     game (5.70%)   \n",
       "\n",
       "         Choice 2          Choice 3         Choice 4          Choice 5  \n",
       "0        (11.98%)       ids (7.52%)     ices (4.99%)       urs (4.02%)  \n",
       "1      al (3.49%)       ier (1.01%)     iers (0.16%)        us (0.08%)  \n",
       "2    for (17.40%)      term (5.95%)     name (5.18%)     words (2.47%)  \n",
       "3     all (1.54%)         a (1.45%)      our (0.75%)      this (0.69%)  \n",
       "4    time (1.14%)     world (1.08%)   modern (0.92%)   ancient (0.71%)  \n",
       "5      , (22.62%)        's (8.07%)      and (5.68%)     world (4.95%)  \n",
       "6   They (11.45%)       The (8.13%)       In (2.92%)        It (2.64%)  \n",
       "7     The (0.04%)         A (0.02%)        I (0.01%)        In (0.01%)  \n",
       "8      In (3.43%)  Contents (3.10%)        A (2.06%)      They (1.84%)  \n",
       "9   first (1.94%)      main (1.30%)   player (1.04%)         \" (0.96%)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#input_txt = \"Transformers are the\"\n",
    "input_txt = \"Transformers are the \"                    # This one is interesting to see how things change\n",
    "#input_txt = \"Transformers are built using the\"         # Note the word pieces\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 10\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        \n",
    "        # Select logits of the first batch and the last token and apply softmax to get the probability\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        \n",
    "        # Store tokens with highest probabilities in our little table\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        iterations.append(iteration)\n",
    "\n",
    "            \n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "\n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The generate method runs the transformer several steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers are the vernacular of the game.\n",
      "\n",
      "The game\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
      "\n",
      "\n",
      "\"The unicorns were very intelligent, and they were very intelligent,\" said Dr. David S. Siegel, a professor of anthropology at the University of California, Berkeley. \"They were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very intelligent, and they were very\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "input_txt = \"\"\"In a shocking finding, scientist discovered \\\n",
    "a herd of unicorns living in a remote, previously unexplored \\\n",
    "valley, in the Andes Mountains. Even more surprising to the \\\n",
    "researchers was the fact that the unicorns spoke perfect English.\\n\\n\n",
    "\"\"\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play around with sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some scoring functions\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n",
    "\n",
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:])\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n",
      "\n",
      "log-prob: -38.90\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, greedy_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(greedy_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure if I'll\n",
      "\n",
      "log-prob: -37.01\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, beam_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(beam_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "\n",
      "log-prob: -52.53\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, beam_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(beam_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling within the probability distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "torch.random.manual_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog for the rest of the day, but this had me staying in an unusual room and not going on nights out with friends (which will always be wondered for a mere minute or so at this point).\n",
      "\n",
      "I\n",
      "\n",
      "log-prob: -132.17\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, sample_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(sample_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog and playing around with him. I have a strong desire to have children, and I want them to be as healthy as possible. We do not mind having the dog play around in the backyard or outside. I really\n",
      "\n",
      "log-prob: -97.50\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, sample_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(sample_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-K sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog,\" said Boudreau.\n",
      "\n",
      "Other breeds, such as dogs and dogs with arthritis, also have been found to have increased fitness.\n",
      "\n",
      "\"Our dog is very intelligent and enjoys to be around people,\n",
      "\n",
      "log-prob: -102.27\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, sample_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(sample_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top-P sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I enjoy walking with my cute dog and talking to him because he reminds me so much of me. I know he'll pick up on the little pictures and love the way he looks and I want to show him things that he doesn't find so hilarious\n",
      "\n",
      "log-prob: -105.89\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, sample_output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(sample_output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Top-K and Top-P Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=25, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  2883,  6155,   351,   616, 13779,  3290,   287,   616,  1310,\n",
       "          1363,    13,   679,  1595,   470,   804,   881,   588,   502,   379,\n",
       "           477,    13,   314,  1107,  9144,   465,  3241,   284,  3703,   523,\n",
       "           314,   655,   460,   470,  1037,   475,  8212,    11,   475,   340,\n",
       "           318,  1107,  1593,   284,   502,   326,   339,  1595,   470,   787],\n",
       "        [   40,  2883,  6155,   351,   616, 13779,  3290,   526,   628,   198,\n",
       "           464,  3290,   290,   262,  1545,  1718,   257,  2513,   286,   511,\n",
       "           898,   706,   484,  5284,   379,   262,  1363,    13,   628,   198,\n",
       "             1,  1026,   373,  1611,   286, 13779,   553,   531,  4767,    89,\n",
       "            13,   198,   198,   464,  3155,   338,  3290,   550,   550,   257],\n",
       "        [   40,  2883,  6155,   351,   616, 13779,  3290,   284,  8073,    13,\n",
       "           887,   314,   892,   340,   338,  1107,  1593,   329,   257,  1048,\n",
       "           588,   502,   284,   651,   284,   760,   606,   355,   881,   355,\n",
       "          1744,    13,   632,  1724,   326,   356,   460,  2193,   422,  1123,\n",
       "           584,    11,   290,   356,   460,  2648,   617,   286,   674,   898]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog in my little home. He doesn't look much like me at all. I really appreciate his attention to detail so I just can't help but smile, but it is really important to me that he doesn't make\n",
      "\n",
      "log-prob: -280.66\n",
      "\n",
      "1: I enjoy walking with my cute dog.\"\n",
      "\n",
      "\n",
      "The dog and the friend took a walk of their own after they arrived at the home.\n",
      "\n",
      "\n",
      "\"It was kind of cute,\" said Petz.\n",
      "\n",
      "The couple's dog had had a\n",
      "\n",
      "log-prob: -183.95\n",
      "\n",
      "2: I enjoy walking with my cute dog to dinner. But I think it's really important for a person like me to get to know them as much as possible. It means that we can learn from each other, and we can share some of our own\n",
      "\n",
      "log-prob: -81.51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))\n",
    "    logp = sequence_logprob(model, sample_outputs, input_len=len(input_ids[0]))\n",
    "    sample_outputs = sample_outputs[1:, :]\n",
    "    print(f\"\\nlog-prob: {logp:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_gpt(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=100, \n",
    "        top_k=25, \n",
    "        top_p=0.95\n",
    "    )\n",
    "    print(tokenizer.decode(sample_outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love to eat pizza all day,\" he tells me, with an amused snort. \"And I know I should be better at it. Because I know, like, when you eat something, you'll get a kick out of it.\"\n",
      "\n",
      "He looks back at the pizza, which was his favorite, and takes a bite. \"But when I get home…\n"
     ]
    }
   ],
   "source": [
    "do_gpt(\"I love to eat pizza all day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's the end of the world, it's what I do. I want to live life and make money and not feel like I'm not doing good things. I want people to see I'm not doing what they want to see. I want people to go out and see the things that I've done that have made the world better and the way I'm doing it, I want to make sure that those people know I don't care about anything.\"\n",
      "\n",
      "In an interview with The Associated\n"
     ]
    }
   ],
   "source": [
    "do_gpt(\"It's the end of the world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My cat turned to me with a murderous look in his eyes. \"Are you going to tell me anything about what you're up to?\" \"No, I think you're going to tell me something about what I've been up to for a long time,\" she said. \"This is the first time, though, I've heard any real information about who my cat is and what it's capable of doing.\" \"I'll do anything to help,\" I said. Her face hardened in embarrassment.\n"
     ]
    }
   ],
   "source": [
    "do_gpt(\"My cat turned to me with a murderous look in his eyes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
